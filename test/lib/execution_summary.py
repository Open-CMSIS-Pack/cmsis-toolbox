# This script is designed to process the output.xml file generated by Robot
# Framework, extract the test statuses, and create a markdown file based
# on the parsed information.
# 
# execution_summary.py <path to input output.xml> [output file summary.md]

import os
import re
import argparse
import sys
import glob
from pathlib import Path
from robot.api import ExecutionResult, ResultVisitor
from robot.result.model import TestCase
from robot.result.executionresult import Result 
from reference_compare import *

class ResultVisitorEx(ResultVisitor):
    def __init__(self, test_env_files_path:str, output_path:str, markdown_file:str):
        self.failed_tests = {}
        self.passed_tests = {}
        self.skipped_tests = {}
        self.test_env_files_path=test_env_files_path
        self.markdown_file = markdown_file
        self.output_path = output_path

        # Remove existing markdown file if it exists
        if os.path.exists(markdown_file):
            os.remove(markdown_file)

    def visit_test(self, test: TestCase):
        tags = ", ".join(test.tags)
        duration = "{:.2f} s".format(test.elapsed_time.total_seconds())
        status = (test.name, test.message, duration, test.parent.name)
        test_status = self.failed_tests if test.status == 'FAIL' else (
            self.passed_tests if test.status == 'PASS' else self.skipped_tests)
        if tags not in test_status:
            test_status[tags] = []
        test_status[tags].append(status)

    def read_file_content(self, file_path):
        with open(file_path, 'r') as file:
            return file.read()

    def extract_platform(self, file_name):
        match = re.search(r'test-env-(.*)\.md', file_name)
        return match.group(1) if match else None
    
    def get_test_env(self, f):
        test_env_files = glob.glob(self.test_env_files_path + '/**/test-env-*.md', recursive=False)
        # return if no test_env file found
        if not test_env_files:
            print("No test-env-*.md files found")
            return

        # Read the content of the first file
        first_file_content = self.read_file_content(test_env_files[0])

        has_same_env = True
        f.write("## Test Environment\n\n")
        test_env_content = ""

        # Iterate through the rest of the files and compare content
        for file_path in test_env_files[1:]:
            file_name = Path(file_path).name
            platform = self.extract_platform(file_name)
            if platform:
                test_env_content += f"\n## {platform}\n\n"
            
            current_file_content = self.read_file_content(file_path)
            test_env_content += current_file_content
            
            if current_file_content != first_file_content:
                has_same_env &= False

        if has_same_env:
            test_env_content = first_file_content

        f.write(test_env_content + "\n")

    def end_result(self, result: Result):
        with open(self.markdown_file, "w") as f:
            f.write("# Robot Framework Report\n\n")
            self.get_test_env(f)
            f.write("## Summary\n\n")
            f.write("|:white_check_mark: Passed|:x: Failed|:fast_forward: Skipped|Total|\n")
            f.write("|:----:|:----:|:-----:|:---:|\n")
            f.write(f"|{result.statistics.total.passed}|{result.statistics.total.failed}|{result.statistics.total.skipped}|{result.statistics.total.total}|\n")
            self.__write_test_section(f, self.passed_tests, "Passed Tests", "|Tag|Test|:clock1030: Duration|Suite|\n")
            self.__write_test_section(f, self.failed_tests, "Failed Tests", "|Tag|Test|Message|:clock1030: Duration|Suite|\n")
            self.__write_test_section(f, self.skipped_tests, "Skipped Tests", "|Tag|Test|Suite|\n")


    def __write_test_section(self, file, test_dict, section_header, table_header):
        if len(test_dict) != 0:
            file.write(f"\n## {section_header}\n\n")
            file.write(table_header)
            tokens = table_header.split('|')
            table_sep = "|:-"
            num_req_sep = len(tokens) - 2
            for i in range(num_req_sep):
                table_sep += "--:|" 
                if i != (num_req_sep - 1):
                    table_sep += ":-" 
            file.write(table_sep + "\n")
            for key, value in test_dict.items():
                for name, msg, duration, suite in value:
                    if section_header.startswith("Pass"):
                        file.write(f"|{key}|{name}|{duration}|{suite}|\n")
                    elif section_header.startswith("Fail"):
                        file.write(f"|{key}|{name}|{msg}|{duration}|{suite}|\n")
                    elif section_header.startswith("Skip"):
                        file.write(f"|{key}|{name}|{suite}|\n")

def main():
    parser = argparse.ArgumentParser(description='Consolidate test summary report')
    parser.add_argument('test_env_files_path', type=str, help='Path to the test environment files')
    parser.add_argument('-r', '--reference_file', type=str, help='Path to reference file')
    parser.add_argument('-o', '--output_file', type=str, nargs='?', default='output.xml', help='Path to output xml file')
    parser.add_argument('-m', '--markdown_file', type=str, nargs='?', default='summary_report.md', help='Path to consolidated summary markdown file')
    args = parser.parse_args()

    # generate summary report
    result = ExecutionResult(args.output_file)
    result.visit(ResultVisitorEx(args.test_env_files_path, args.output_file, args.markdown_file))

    # compare test execution summary and reference results 
    return compare_summaries(args.markdown_file, args.reference_file)

if __name__ == '__main__':
    try:
        sys.exit(main())
    except Exception as e:
        print(f'An error occurred: {e}', file=sys.stderr)
        sys.exit(1)
